import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import mutual_info_regression
from xgboost import XGBRegressor
import warnings

warnings.filterwarnings("ignore")

def load_excel(file, transpose=False):
    df = pd.read_excel(file)
    if transpose:
        df = df.T
        df.columns = df.iloc[0]
        df = df.drop(df.index[0])
    df = df.reset_index(drop=True)
    return df.astype(float)

def hybrid_linearity_test(df, features, target, spearman_threshold=0.3, mi_threshold=0.01):
    linear_feats = []
    nonlinear_feats = []
    spearman_corrs = df[features + [target]].corr(method="spearman")[target].drop(target)
    mi = mutual_info_regression(df[features], df[target])
    for i, f in enumerate(features):
        if abs(spearman_corrs[f]) >= spearman_threshold:
            linear_feats.append(f)
        elif mi[i] >= mi_threshold:
            nonlinear_feats.append(f)
    if not linear_feats and not nonlinear_feats:
        nonlinear_feats = features
    return linear_feats, nonlinear_feats, spearman_corrs, pd.Series(mi, index=features)

def train_and_evaluate(df, feature_cols, target_col):
    x, y = df[feature_cols], df[target_col]
    linear_feats, nonlinear_feats, spearman_corrs, mi = hybrid_linearity_test(df, feature_cols, target_col)

    x_train_full, x_test, y_train_full, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.25, random_state=42)

    lm = Pipeline([("scaler", StandardScaler()), ("regressor", LinearRegression())]) if linear_feats else None
    rf = Pipeline([("scaler", StandardScaler()), ("regressor", RandomForestRegressor(n_estimators=100, random_state=42))]) if nonlinear_feats else None

    if lm:
        lm.fit(x_train[linear_feats], y_train)
    if rf:
        rf.fit(x_train[nonlinear_feats], y_train)

    lin_pred_val = lm.predict(x_val[linear_feats]) if lm else np.zeros(len(y_val))
    rf_pred_val = rf.predict(x_val[nonlinear_feats]) if rf else np.zeros(len(y_val))
    blend_val = np.vstack([lin_pred_val, rf_pred_val]).T

    meta = XGBRegressor(objective="reg:squarederror", random_state=42)
    meta.fit(blend_val, y_val)

    lin_pred_test = lm.predict(x_test[linear_feats]) if lm else np.zeros(len(y_test))
    rf_pred_test = rf.predict(x_test[nonlinear_feats]) if rf else np.zeros(len(y_test))
    blend_test = np.vstack([lin_pred_test, rf_pred_test]).T

    final_preds = meta.predict(blend_test)

    cv_r2 = None
    if len(x_train_full) >= 5:
        def cv_blend(x, y):
            scores = []
            kf = KFold(n_splits=5, shuffle=True, random_state=42)
            for tr, val in kf.split(x):
                xm, xv = x.iloc[tr], x.iloc[val]
                ym, yv = y.iloc[tr], y.iloc[val]

                lm2 = Pipeline([("scaler", StandardScaler()), ("regressor", LinearRegression())]) if linear_feats else None
                rf2 = Pipeline([("scaler", StandardScaler()), ("regressor", RandomForestRegressor(n_estimators=100))]) if nonlinear_feats else None

                if lm2:
                    lm2.fit(xm[linear_feats], ym)
                if rf2:
                    rf2.fit(xm[nonlinear_feats], ym)

                lp = lm2.predict(xv[linear_feats]) if lm2 else np.zeros(len(yv))
                rp = rf2.predict(xv[nonlinear_feats]) if rf2 else np.zeros(len(yv))

                blend = np.vstack([lp, rp]).T
                xgb2 = XGBRegressor(**meta.get_params())
                xgb2.fit(blend, yv)
                scores.append(r2_score(yv, xgb2.predict(blend)))
            return np.mean(scores)
        cv_r2 = cv_blend(x_train_full, y_train_full)

    return {
        "linear_model": lm,
        "rf_model": rf,
        "meta_model": meta,
        "linear_features": linear_feats,
        "nonlinear_features": nonlinear_feats,
        "x_test": x_test,
        "y_test": y_test,
        "final_preds": final_preds,
        "cv_r2": cv_r2,
        "spearman": spearman_corrs,
        "mutual_info": mi,
    }

def color_metric(value, metric_type, y_true=None):
    if y_true is not None:
        std = np.std(y_true)
        if metric_type == "mse":
            ratio = value / (std**2)
            color = "green" if ratio < 0.1 else "orange" if ratio < 0.5 else "red"
        elif metric_type == "mae":
            ratio = value / std
            color = "green" if ratio < 0.3 else "orange" if ratio < 0.7 else "red"
        else:
            color = "green" if value > 0.8 else "orange" if value > 0.5 else "red"
    else:
        color = "green" if value > 0.8 else "orange" if value > 0.5 else "red"
    return f"{value:.4f} ({color})"

def main():
    print("Hybrid Model Predictor\n")
    file_path = input("Enter Excel file path: ")
    transpose = input("Transpose data? (y/n): ").lower() == 'y'

    try:
        df = load_excel(file_path, transpose=transpose)
    except Exception as e:
        print(f"Error loading Excel file: {e}")
        return

    print("\nColumns found:", list(df.columns))
    target = input("Select target column: ")
    features = input("Enter feature columns (comma separated): ").split(',')

    features = [f.strip() for f in features if f.strip() != target]

    if not features:
        print("You must provide at least one feature.")
        return

    model = train_and_evaluate(df, features, target)

    mse = mean_squared_error(model["y_test"], model["final_preds"])
    mae = mean_absolute_error(model["y_test"], model["final_preds"])
    r2 = r2_score(model["y_test"], model["final_preds"])
    cv_r2 = model["cv_r2"]

    print("\nPerformance metrics:")
    print(f"MSE: {color_metric(mse, 'mse', model['y_test'])}")
    print(f"MAE: {color_metric(mae, 'mae', model['y_test'])}")
    print(f"R²: {color_metric(r2, 'r2')}")
    if cv_r2 is not None:
        print(f"CV R²: {color_metric(cv_r2, 'cv_r2')}")

    print("\nEnter values for prediction:")
    input_features = {}
    for f in features:
        val = float(input(f"{f}: "))
        input_features[f] = val

    user_df = pd.DataFrame([input_features])
    lp = model["linear_model"].predict(user_df[model["linear_features"]])[0] if model["linear_model"] else 0.0
    rp = model["rf_model"].predict(user_df[model["nonlinear_features"]])[0] if model["rf_model"] else 0.0
    final_pred = model["meta_model"].predict(np.array([[lp, rp]]))[0]

    print(f"\nPredicted {target}: {final_pred:.4f}")

    correct = input("If you want to submit a correction, enter the corrected value or press Enter to skip: ")
    if correct.strip():
        try:
            corrected_val = float(correct.strip())
            new_row = {**input_features, target: corrected_val}
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
            print("Correction saved. Retraining model with new data...\n")
            model = train_and_evaluate(df, features, target)
            print("Retraining completed.")
        except Exception as e:
            print(f"Error with correction input: {e}")

if __name__ == "__main__":
    main()
